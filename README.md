ğŸš€ Gemini RAG Agent with Memory

A Production-Ready Retrieval-Augmented Generation (RAG) System using Google Gemini, LangChain, ChromaDB & Streamlit

ğŸ“Œ Overview

This project implements a fully functional RAG pipeline using:

Google Gemini (2.5 Flash / Pro)

LangChain for orchestration

Sentence Transformers for text embeddings

ChromaDB as the vector store

Streamlit for frontend UI

Conversation Memory for long chats

The agent can:

âœ”ï¸ Accept document uploads
âœ”ï¸ Index & embed documents automatically
âœ”ï¸ Retrieve relevant chunks
âœ”ï¸ Answer questions grounded in your documents
âœ”ï¸ Maintain conversation context
âœ”ï¸ Provide traceable sources

Perfect for interviews, portfolios, real-world use, and learning RAG deeply.

ğŸ§  Key Features
ğŸ” RAG (Retrieval-Augmented Generation)

Your questions are answered using your documentsâ€”not just the modelâ€™s training.

ğŸ“ Document Upload in UI

Upload pdf, docx, txt, or md files directly in Streamlit.

ğŸ§© Automatic Ingestion Pipeline

On upload, the system:

Loads the document

Splits text into chunks

Embeds using Sentence Transformers

Stores vectors in ChromaDB

ğŸ’¬ Conversation Memory

The agent remembers previous messages using ConversationSummaryMemory.

âš¡ Works Offline After Indexing

ChromaDB persists locally.

ğŸ§± Clean Modular Code (Best Practices)

Separate modules:

utils.py â†’ embeddings + vector store
ingest.py â†’ ingestion pipeline
rag_chain.py â†’ RAG chain builder
memory.py â†’ chat memory
app.py â†’ Streamlit UI

ğŸ—ï¸ System Architecture
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚          Streamlit UI          â”‚
          â”‚  â€¢ Chat Box                    â”‚
          â”‚  â€¢ Document Upload             â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚    Ingestion Pipeline   â”‚
             â”‚  â€¢ Load documents       â”‚
             â”‚  â€¢ Split text           â”‚
             â”‚  â€¢ Generate embeddings  â”‚
             â”‚  â€¢ Store in ChromaDB    â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚     Vector Retriever (K)     â”‚
           â”‚  Fetch relevant document     â”‚
           â”‚  chunks from ChromaDB        â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚        Gemini LLM (2.5)        â”‚
          â”‚  â€¢ Combines retrieved chunks   â”‚
          â”‚  â€¢ Produces grounded answers   â”‚
          â”‚  â€¢ Maintains conversation mem  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Final Answer    â”‚
                â”‚  + Source Citationsâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“‚ Project Structure
gemini_rag_agent/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py              # Streamlit frontend
â”‚   â”œâ”€â”€ utils.py            # Embeddings, ChromaDB utils
â”‚   â”œâ”€â”€ rag_chain.py        # RAG chain builder
â”‚   â”œâ”€â”€ memory.py           # Conversation memory
â”‚   â”œâ”€â”€ ingest.py           # Ingestion pipeline
â”‚
â”œâ”€â”€ data/                   # Uploaded documents (ignored in Git)
â”‚
â”œâ”€â”€ storage/chroma/         # Vector DB (ignored in Git)
â”‚
â”œâ”€â”€ .env.example            # Sample environment vars
â”œâ”€â”€ .gitignore              # Ignore sensitive files & local DB
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ README.md               # Project documentation

âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone repository
git clone https://github.com/akasherinhikkal-code/gemini_rag_agent.git
cd gemini_rag_agent

2ï¸âƒ£ Create a virtual environment
python3 -m venv .venv
source .venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Configure API Key

Create .env file:

GOOGLE_API_KEY=YOUR_KEY_HERE


Get your key from:
https://aistudio.google.com/apikey

5ï¸âƒ£ Run document ingestion (optional)

If you placed documents in /data manually:

python app/ingest.py


Otherwise, you can upload them directly from UI.

6ï¸âƒ£ Start the Streamlit app
streamlit run app/app.py


ğŸ‘‰ Opens at:
http://localhost:8501

ğŸ§ª Sample Query Flow

Upload a PDF â†’ Ask:

ğŸ‘‰ "Summarize the document."
ğŸ‘‰ "What are the key sections?"
ğŸ‘‰ "Compare chapter 2 and chapter 5."

The agent replies with document-grounded answers + sources.

ğŸ¯ Use Cases

âœ”ï¸ Resume-based Q&A
âœ”ï¸ Company policy search
âœ”ï¸ Legal document summarization
âœ”ï¸ Technical manual Q&A
âœ”ï¸ Finance reports question answering
âœ”ï¸ Student study assistant
âœ”ï¸ Private knowledge-base chatbot

ğŸ›¡ï¸ Security

.env, /data, /storage are all ignored in GitHub

No personal keys or info will be committed

ChromaDB persists locally only

ğŸ› ï¸ Future Improvements

ğŸ“Œ Add session-based private indices
ğŸ“Œ Support for images + OCR ingestion
ğŸ“Œ Cloud vector database (Pinecone / Weaviate)
ğŸ“Œ Prompt caching
ğŸ“Œ Multi-document analytics

ğŸ¤ Contributing

Pull requests are welcome!
If you want new features, feel free to open an issue.