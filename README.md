ğŸ¤– Gemini RAG Agent with Memory

AI-powered Retrieval-Augmented Generation (RAG) system built with Google Gemini, LangChain, and ChromaDB.
This project lets users upload documents, build a knowledge base, and ask questions grounded in those documents â€” all through a clean Streamlit UI.

ğŸŒŸ Features
âœ… RAG Pipeline (Retrieval + Generation)

Extracts knowledge from uploaded documents

Uses semantic search over ChromaDB

Generates accurate, grounded answers using Google Gemini

âœ… Document Uploading (Streamlit)

Upload PDF, TXT, DOCX, MD files

Automatic ingestion + re-indexing

Real-time update of vector database

âœ… Memory-Enabled Conversation

Uses ConversationSummaryMemory to maintain context across chat sessions.

âœ… Google Gemini Integration

Supports:

models/gemini-2.5-flash

models/gemini-2.5-pro

âœ… ChromaDB Vector Store

Persistent storage

Automatically rebuilt when new docs are uploaded

âœ… HuggingFace Sentence Embeddings

Uses lightweight + high-performance encoder:

all-MiniLM-L6-v2

ğŸ—ï¸ System Architecture

Here is a visual explanation of the full workflow:

                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚      Streamlit UI        â”‚
                 â”‚  - Chat Input            â”‚
                 â”‚  - Document Upload       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                Document Upload Event
                             â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚     Ingestion Pipeline    â”‚
                 â”‚  - Load Documents         â”‚
                 â”‚  - Split Text             â”‚
                 â”‚  - Embeddings (HF)        â”‚
                 â”‚  - Store in ChromaDB      â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
               RAG Query (User Question)
                             â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Retriever (ChromaDB)     â”‚
                 â”‚  Fetch Top-K Relevant     â”‚
                 â”‚  Document Chunks          â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Google Gemini LLM         â”‚
                 â”‚ - RAG Prompting           â”‚
                 â”‚ - Conversation Memory     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚      Final Response        â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ Project Structure
gemini_rag_agent/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                 # Streamlit frontend
â”‚   â”œâ”€â”€ utils.py               # Embeddings, Chroma DB helpers
â”‚   â”œâ”€â”€ rag_chain.py           # RAG chain builder
â”‚   â”œâ”€â”€ memory.py              # Conversation memory
â”‚   â””â”€â”€ ingest.py              # Document ingestion pipeline
â”‚
â”œâ”€â”€ data/                      # User documents
â”œâ”€â”€ storage/chroma/            # Chroma vector DB (ignored in git)
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
